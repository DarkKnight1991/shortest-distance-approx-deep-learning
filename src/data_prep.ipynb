{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook prepares the input data to be consumed by the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to extract the edgelist from .mtx file (the way Node2Vec expects the input). We will iterate through all the mtx files and create a corresponding .edgelist file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_files = os.listdir('../data')\n",
    "for file in data_files:\n",
    "    if file.endswith('.mtx'):\n",
    "        file_name = file.replace('.mtx', '')\n",
    "        file_edgelist = file_name+'.edgelist'\n",
    "        if not file_edgelist in data_files:\n",
    "            lines = None\n",
    "            with open('../data/'+file) as file_mtx:\n",
    "                lines = file_mtx.readlines()\n",
    "            with open('../data/'+file_edgelist, 'w') as file_edgelist:\n",
    "                file_edgelist.writelines(lines[2:])\n",
    "                print(file_edgelist, 'created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted edgelists from .mtx files using above cell, let's generate node embeddings (node2vec). For that I will use the code that the author's have shared on there [Github](https://github.com/aditya-grover/node2vec). But first we have to convert the script from python2 to python3 and replace \"import node2vec\" with \"import node2vec3\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! 2to3 -w './node2vec'  # didn't work. No such file or directory error\n",
    "# os.listdir('./node2vec')  # while this works!\n",
    "# so converted main.py and node2vec.py using online translators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generating embeddings use the parameters used in the \"shortest path distance\" paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Walk iteration:\n1 / 10\n2 / 10\n3 / 10\n4 / 10\n5 / 10\n6 / 10\n7 / 10\n8 / 10\n9 / 10\n10 / 10\nWall time: 18min 24s\n"
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "if not os.path.exists('../data/emb'):\n",
    "    os.makedirs('../data/emb')\n",
    "# ! python node2vec/main3.py --help\n",
    "! python node2vec/main3.py --input ../data/socfb-OR.edgelist --output ../data/emb/socfb-OR.emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|█████████▉| 6370/6386 [00:15<00:00, 408.14it/s]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0., 3., 3., 2., 3., 3., 2., 3., 2., 2., 2., 2., 2., 3., 3., 2., 3.,\n       4., 2., 2., 2., 2., 2., 2., 3., 2., 2., 3., 2., 2., 3., 3., 2., 3.,\n       2., 3., 3., 2., 3., 2., 2., 3., 2., 2., 3., 3., 3., 2., 3., 3.])"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from graph_proc import Graph\n",
    "from logger import Logger\n",
    "\n",
    "logger = Logger('../outputs/logs', 'log_')\n",
    "graph = Graph('../data/socfb-American75.mtx', logger)\n",
    "graph.calculate_distances(1)  # there is an isolated cycle in the graph! 1187-5780, that's why it keeps looping\n",
    "graph.distance_map[1][0:50]\n",
    "# total accessible nodes 6370; each source processing ~13 seconds; 6370*13 seconds = 23 hours! (good job :-p )\n",
    "# but we don't need to process all the nodes, we need ~100. So 21 mins! But still need to optimize BFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is saved in a pickle file (dict) to analyse. As you can see below only isolated (disconnected from source) nodes are left out which for a cycle with another node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1187 -- [5780]\n1460 -- [1599]\n1571 -- [5399]\n1599 -- [1460]\n1790 -- [5975]\n3427 -- [4080]\n4080 -- [3427]\n4166 -- [5418]\n4681 -- [4849]\n4849 -- [4681]\n4867 -- [5183]\n5183 -- [4867]\n5399 -- [1571]\n5418 -- [4166]\n5780 -- [1187]\n5975 -- [1790]\n"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "\n",
    "mtx_path = '..\\data\\socfb-American75.mtx'\n",
    "mat_csr = io.mmread(mtx_path).tocsr()\n",
    "distance_map = pickle.load(open('../outputs/distance_map_1588710068.714855.pickle', 'rb'))\n",
    "l = np.array(distance_map[1])\n",
    "hitlist = np.where(l==np.inf)[0]\n",
    "for i in hitlist:\n",
    "    print(i, '--', np.where(mat_csr[i].toarray()[0]>0)[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}