{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook prepares the input data to be consumed by the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to extract the edgelist from .mtx file (the way Node2Vec expects the input). We will iterate through all the mtx files and create a corresponding .edgelist file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_files = os.listdir('../data')\n",
    "for file in data_files:\n",
    "    if file.endswith('.mtx'):\n",
    "        file_name = file.replace('.mtx', '')\n",
    "        file_edgelist = file_name+'.edgelist'\n",
    "        if not file_edgelist in data_files:\n",
    "            lines = None\n",
    "            with open('../data/'+file) as file_mtx:\n",
    "                lines = file_mtx.readlines()\n",
    "            with open('../data/'+file_edgelist, 'w') as file_edgelist:\n",
    "                file_edgelist.writelines(lines[2:])\n",
    "                print(file_edgelist, 'created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted edgelists from .mtx files using above cell, let's generate node embeddings (node2vec). For that I will use the code that the author's have shared on there [Github](https://github.com/aditya-grover/node2vec). But first we have to convert the script from python2 to python3 and replace \"import node2vec\" with \"import node2vec3\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! 2to3 -w './node2vec'  # didn't work. No such file or directory error\n",
    "# os.listdir('./node2vec')  # while this works!\n",
    "# so converted main.py and node2vec.py using online translators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generating embeddings use the parameters used in the \"shortest path distance\" paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n",
      "Wall time: 18min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "if not os.path.exists('../data/emb'):\n",
    "    os.makedirs('../data/emb')\n",
    "# ! python node2vec/main3.py --help\n",
    "! python node2vec/main3.py --input ../data/socfb-OR.edgelist --output ../data/emb/socfb-OR.emd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Graph class has a <i>naive</i> implementation of Dijkstra's Algorithm to calculate distance of all the nodes from a specified source node. It is slow but since we need to run it for landmarks (number of landmarks << number of nodes) only I will go ahead with this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0%|          | 0/150 [00:00<?, ?it/s]number of landmarks: 150\n100%|██████████| 150/150 [37:05<00:00, 14.83s/it]save path: ../outputs/distance_map_1588792161.8904061.pickle\n\n"
    }
   ],
   "source": [
    "from graph_proc import Graph\n",
    "from logger import Logger\n",
    "\n",
    "logger = Logger('../outputs/logs', 'log_')\n",
    "graph = Graph('../data/socfb-American75.mtx', logger)\n",
    "save_path = graph.process_landmarks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is saved in a pickle file (dict) to analyse. As you can see below only isolated (disconnected from source) nodes are left out, which form a cycle with another node (isolated cycles). Same set of isolated nodes are found for all of the landmarks. So we can ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of sources for which any isolated nodes found are 150\n"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "\n",
    "save_path = '../outputs/distance_map_1588792161.8904061.pickle'\n",
    "mtx_path = '..\\data\\socfb-American75.mtx'\n",
    "mat_csr = io.mmread(mtx_path).tocsr()\n",
    "distance_map = pickle.load(open(save_path, 'rb'))\n",
    "keys = list(distance_map.keys())\n",
    "count = 0\n",
    "for key in keys:\n",
    "    l = np.array(distance_map[key])\n",
    "    hitlist = np.where(l==np.inf)[0]\n",
    "    # print('Number of isolated keys for source-{} is {}'.format(key, len(hitlist)))\n",
    "    if(len(hitlist) > 0):\n",
    "        count += 1\n",
    "    # for i in hitlist:\n",
    "    #     print(i, '--', np.where(mat_csr[i].toarray()[0]>0)[0])\n",
    "    # if(len(hitlist)>0):\n",
    "    #     break\n",
    "print('Number of sources for which any isolated nodes found are', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All cells before this had to be run once to process the graph and save results to save time. Now we have to read the distance map and embeddings to form training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "size of emd_map: 0.28133392333984375 MB\nsize of distance_map: 0.00447845458984375 MB\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "save_path = '../outputs/distance_map_1588792161.8904061.pickle'\n",
    "distance_map = pickle.load(open(save_path, 'rb'))\n",
    "emd_path = '../data/emb/socfb-American75.emd'\n",
    "emd_map = {}\n",
    "with open(emd_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines[1:]:\n",
    "        temp = line.split(' ')\n",
    "        emd_map[np.int(temp[0])] = np.array(temp[1:], dtype=np.float)\n",
    "print('size of emd_map:', sys.getsizeof(emd_map)/1024/1024,'MB')\n",
    "print('size of distance_map:', sys.getsizeof(distance_map)/1024/1024,'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 150/150 [00:03<00:00, 45.12it/s]length of embedding-distance pairs 955350\n\n"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "dataset_path = '../data/datasets/socfb-American75.pickle'\n",
    "\n",
    "emd_dist_pair = []\n",
    "for landmark in tqdm(list(distance_map.keys())):\n",
    "    node_distances = distance_map[landmark]\n",
    "    emd_dist_pair.extend([((emd_map[node]+emd_map[landmark])/2, distance) for node, distance in enumerate(node_distances, 1) if node != landmark and distance != np.inf])\n",
    "\n",
    "print('length of embedding-distance pairs', len(emd_dist_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 955350/955350 [00:01<00:00, 613272.82it/s]Shape of x=(955350, 128) and y=(955350,)\nsize of x=932.9590911865234 MB and y=7.2888336181640625 MB\n\n"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "x = np.zeros((len(emd_dist_pair), len(emd_dist_pair[0][0])))\n",
    "y = np.zeros((len(emd_dist_pair),))\n",
    "\n",
    "for i, tup in enumerate(tqdm(emd_dist_pair)):\n",
    "    x[i] = tup[0]\n",
    "    y[i] = tup[1]\n",
    "print(\"Shape of x={} and y={}\".format(x.shape, y.shape))\n",
    "print('size of x={} MB and y={} MB'.format(sys.getsizeof(x)/1024/1024, sys.getsizeof(y)/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('Range of distance values', 1.0, 7.0)"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "'Range of distance values', np.min(y), np.max(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sanity check..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "number of negative distances 0\n"
    }
   ],
   "source": [
    "num_neg_dist = 0\n",
    "distances_ = []\n",
    "for i, landmark in enumerate(distance_map.keys()):\n",
    "    distances_.extend(distance_map[landmark])\n",
    "print('number of negative distances', np.sum(np.array(distances_) < 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data takes up a lot of space, let's convert the datatype of x and y. In case you are worried about the precision loss, I think you can save the converted data into separate ndarray(x1), and try \"np.mean(np.abs(x-x1))\". For this data it was very small (2.7954226433144966e-09),so ignoring it. And in our case graphs are unweighted, so distance would be integer always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "size of x=466.47959899902344 MB and y=3.6444625854492188 MB\n"
    }
   ],
   "source": [
    "x = x.astype('float32')\n",
    "y = y.astype('int')\n",
    "print('size of x={} MB and y={} MB'.format(sys.getsizeof(x)/1024/1024, sys.getsizeof(y)/1024/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "shapes of train, validation, test data (573209, 128) (573209,) (143303, 128) (143303,) (238838, 128) (238838,)\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "seed_random = 9999\n",
    "np.random.seed(seed_random)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_random)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, train_size=0.75, random_state=seed_random, shuffle=True, stratify=y)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, test_size=0.2, train_size=0.8, random_state=seed_random, shuffle=True, stratify=y_train)\n",
    "\n",
    "print('shapes of train, validation, test data', x_train.shape, y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discribed in point-2 in fun.ipyb, the distance values are heavily skewed. So oversampling under-represented samples. But here is the challenge-- how do you oversample a regression dataset? Didn't find any good existing implementations. But in this cases values are all natural numbers and these might as well be classes! So using SMOTE. \n",
    "Note: I am not using classification models because distance might be outside the range of [[1-7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Frequency of distance values before oversampling [1 2 3 4 5 6 7] [  6568 202316 305345  53622   4932    407     19]\nFrequency of distance values after oversampling (array([1, 2, 3, 4, 5, 6, 7]), array([213741, 202316, 305345, 213741, 213741, 213741, 213741],\n      dtype=int64))\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((1576366, 128), (1576366,))"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "values, counts = np.unique(y_train, return_counts=True)\n",
    "print('Frequency of distance values before oversampling', values, counts)\n",
    "max_val = np.max(counts)\n",
    "resample_dict = {1:int(max_val*0.7), 4:int(max_val*0.7), 5:int(max_val*0.7), 6:int(max_val*0.7), 7:int(max_val*0.7)}\n",
    "\n",
    "ros = RandomOverSampler(sampling_strategy=resample_dict, random_state=seed_random)\n",
    "x_train, y_train = ros.fit_resample(x_train, y_train.astype(np.int))\n",
    "\n",
    "print('Frequency of distance values after oversampling', np.unique(y_train, return_counts=True))\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# TODO try standardization and no normalization also and compare result\n",
    "\n",
    "mm_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_train = mm_scaler.fit_transform(x_train)\n",
    "x_cv = mm_scaler.transform(x_cv)\n",
    "x_test = mm_scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size':5000, 'input_size':x_train.shape[1], 'hidden_units_1':256, 'hidden_units_2':100, 'output_size':1, 'lr': 1e-5, 'max_lr':1e-3, 'epochs': 400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "device: cuda:0\n"
    }
   ],
   "source": [
    "from torch.utils import data as torch_data\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "trainset = torch_data.TensorDataset(torch.as_tensor(x_train, dtype=torch.float, device=device), torch.as_tensor(y_train, dtype=torch.float, device=device))\n",
    "train_dl = torch_data.DataLoader(trainset, batch_size=params['batch_size'], drop_last=True)\n",
    "\n",
    "val_dl = torch_data.DataLoader(torch_data.TensorDataset(torch.as_tensor(x_cv, dtype=torch.float, device=device), torch.as_tensor(y_cv, dtype=torch.float, device=device)), batch_size=params['batch_size'], drop_last=True)\n",
    "\n",
    "test_dl = torch_data.DataLoader(torch_data.TensorDataset(torch.as_tensor(x_test, dtype=torch.float, device=device), torch.as_tensor(y_test, dtype=torch.float, device=device)), batch_size=params['batch_size'], drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "model loaded into device= cuda:0\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Linear-1                  [-1, 256]          33,024\n              ReLU-2                  [-1, 256]               0\n            Linear-3                  [-1, 100]          25,700\n              ReLU-4                  [-1, 100]               0\n            Linear-5                    [-1, 1]             101\n              ReLU-6                    [-1, 1]               0\n================================================================\nTotal params: 58,825\nTrainable params: 58,825\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 0.22\nEstimated Total Size (MB): 0.23\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "torch.manual_seed(9999)\n",
    "def get_model():\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(params['input_size'], params['hidden_units_1']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(params['hidden_units_1'], params['hidden_units_2']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(params['hidden_units_2'], params['output_size']),\n",
    "        torch.nn.ReLU(),\n",
    "        # torch.nn.Softplus(),\n",
    "    )\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def poisson_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Custom loss function for Poisson model.\n",
    "    Equivalent Keras implementation for reference:\n",
    "    K.mean(y_pred - y_true * math_ops.log(y_pred + K.epsilon()), axis=-1)\n",
    "    For output of shape (2,3) it return (2,) vector. Need to calculate\n",
    "    mean of that too.\n",
    "    \"\"\"\n",
    "    y_pred = torch.squeeze(y_pred)\n",
    "    loss = torch.mean(y_pred - y_true * torch.log(y_pred+1e-7))\n",
    "    return loss\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "print('model loaded into device=', next(model.parameters()).device)\n",
    "summary(model, input_size=(128, ))\n",
    "\n",
    "lr_reduce_patience = 10\n",
    "lr_reduce_factor = 0.05\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], momentum=0.9, dampening=0, weight_decay=0, nesterov=True)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=params['lr'], alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "\n",
    "# lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_reduce_factor, patience=lr_reduce_patience, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=1e-8, eps=1e-08)\n",
    "lr_sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=params['max_lr'], steps_per_epoch=len(train_dl), epochs=params['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dl):\n",
    "    model.eval()\n",
    "    final_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for data_cv in dl:\n",
    "            inputs, dist_true = data_cv[0], data_cv[1]\n",
    "            count += len(inputs)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, dist_true)\n",
    "            final_loss += loss.item()\n",
    "    return final_loss/len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 > Best val_loss model saved: 22.379\nepoch:0 -> train_loss=2.6378,val_loss=22.37934 - 0.0 minutes 49.0 seconds\n1 > Best val_loss model saved: 21.39\n2 > Best val_loss model saved: 20.3\n3 > Best val_loss model saved: 19.23\n4 > Best val_loss model saved: 18.188\n5 > Best val_loss model saved: 17.182\n6 > Best val_loss model saved: 16.2\n7 > Best val_loss model saved: 15.262\n8 > Best val_loss model saved: 14.371\n9 > Best val_loss model saved: 13.544\n10 > Best val_loss model saved: 12.782\nepoch:10 -> train_loss=1.78365,val_loss=12.78217 - 0.0 minutes 47.0 seconds\n11 > Best val_loss model saved: 12.098\n12 > Best val_loss model saved: 11.485\n13 > Best val_loss model saved: 10.945\n14 > Best val_loss model saved: 10.467\n15 > Best val_loss model saved: 10.039\n16 > Best val_loss model saved: 9.651\n17 > Best val_loss model saved: 9.286\n18 > Best val_loss model saved: 8.94\n19 > Best val_loss model saved: 8.606\n20 > Best val_loss model saved: 8.284\nepoch:20 -> train_loss=1.67202,val_loss=8.28427 - 0.0 minutes 48.0 seconds\n21 > Best val_loss model saved: 7.973\n22 > Best val_loss model saved: 7.675\n23 > Best val_loss model saved: 7.391\n24 > Best val_loss model saved: 7.124\n25 > Best val_loss model saved: 6.875\n26 > Best val_loss model saved: 6.647\n27 > Best val_loss model saved: 6.438\n28 > Best val_loss model saved: 6.248\n29 > Best val_loss model saved: 6.075\n30 > Best val_loss model saved: 5.918\nepoch:30 -> train_loss=1.43736,val_loss=5.91811 - 0.0 minutes 49.0 seconds\n31 > Best val_loss model saved: 5.775\n32 > Best val_loss model saved: 5.644\n33 > Best val_loss model saved: 5.525\n34 > Best val_loss model saved: 5.416\n35 > Best val_loss model saved: 5.315\n36 > Best val_loss model saved: 5.223\n37 > Best val_loss model saved: 5.139\n38 > Best val_loss model saved: 5.06\n39 > Best val_loss model saved: 4.987\n40 > Best val_loss model saved: 4.919\nepoch:40 -> train_loss=1.30893,val_loss=4.91868 - 0.0 minutes 49.0 seconds\n41 > Best val_loss model saved: 4.854\n42 > Best val_loss model saved: 4.794\n43 > Best val_loss model saved: 4.737\n44 > Best val_loss model saved: 4.684\n45 > Best val_loss model saved: 4.633\n46 > Best val_loss model saved: 4.585\n47 > Best val_loss model saved: 4.54\n48 > Best val_loss model saved: 4.497\n49 > Best val_loss model saved: 4.456\n50 > Best val_loss model saved: 4.417\nepoch:50 -> train_loss=1.24114,val_loss=4.41657 - 0.0 minutes 49.0 seconds\n51 > Best val_loss model saved: 4.379\n52 > Best val_loss model saved: 4.344\n53 > Best val_loss model saved: 4.31\n54 > Best val_loss model saved: 4.278\n55 > Best val_loss model saved: 4.247\n56 > Best val_loss model saved: 4.218\n57 > Best val_loss model saved: 4.19\n58 > Best val_loss model saved: 4.163\n59 > Best val_loss model saved: 4.137\n60 > Best val_loss model saved: 4.112\nepoch:60 -> train_loss=1.1987,val_loss=4.11209 - 0.0 minutes 48.0 seconds\n61 > Best val_loss model saved: 4.088\n62 > Best val_loss model saved: 4.065\n63 > Best val_loss model saved: 4.043\n64 > Best val_loss model saved: 4.021\n65 > Best val_loss model saved: 4.001\n66 > Best val_loss model saved: 3.98\n67 > Best val_loss model saved: 3.961\n68 > Best val_loss model saved: 3.943\n69 > Best val_loss model saved: 3.924\n70 > Best val_loss model saved: 3.907\nepoch:70 -> train_loss=1.16848,val_loss=3.90722 - 0.0 minutes 49.0 seconds\n71 > Best val_loss model saved: 3.89\n72 > Best val_loss model saved: 3.874\n73 > Best val_loss model saved: 3.858\n74 > Best val_loss model saved: 3.843\n75 > Best val_loss model saved: 3.828\n76 > Best val_loss model saved: 3.813\n77 > Best val_loss model saved: 3.799\n78 > Best val_loss model saved: 3.785\n79 > Best val_loss model saved: 3.772\n80 > Best val_loss model saved: 3.759\nepoch:80 -> train_loss=1.14519,val_loss=3.75853 - 0.0 minutes 49.0 seconds\n81 > Best val_loss model saved: 3.746\n82 > Best val_loss model saved: 3.733\n83 > Best val_loss model saved: 3.721\n84 > Best val_loss model saved: 3.709\n85 > Best val_loss model saved: 3.698\n86 > Best val_loss model saved: 3.687\n87 > Best val_loss model saved: 3.675\n88 > Best val_loss model saved: 3.665\n89 > Best val_loss model saved: 3.654\n90 > Best val_loss model saved: 3.644\nepoch:90 -> train_loss=1.12491,val_loss=3.64352 - 0.0 minutes 49.0 seconds\n91 > Best val_loss model saved: 3.633\n92 > Best val_loss model saved: 3.623\n93 > Best val_loss model saved: 3.613\n94 > Best val_loss model saved: 3.603\n95 > Best val_loss model saved: 3.593\n96 > Best val_loss model saved: 3.584\n97 > Best val_loss model saved: 3.574\n98 > Best val_loss model saved: 3.565\n99 > Best val_loss model saved: 3.555\n100 > Best val_loss model saved: 3.546\nepoch:100 -> train_loss=1.1061,val_loss=3.54634 - 0.0 minutes 48.0 seconds\n101 > Best val_loss model saved: 3.537\n102 > Best val_loss model saved: 3.528\n103 > Best val_loss model saved: 3.519\n104 > Best val_loss model saved: 3.51\n105 > Best val_loss model saved: 3.501\n106 > Best val_loss model saved: 3.492\n107 > Best val_loss model saved: 3.482\n108 > Best val_loss model saved: 3.473\n109 > Best val_loss model saved: 3.465\n110 > Best val_loss model saved: 3.456\nepoch:110 -> train_loss=1.08827,val_loss=3.45608 - 0.0 minutes 49.0 seconds\n111 > Best val_loss model saved: 3.448\n112 > Best val_loss model saved: 3.44\n113 > Best val_loss model saved: 3.432\n114 > Best val_loss model saved: 3.424\n115 > Best val_loss model saved: 3.417\n116 > Best val_loss model saved: 3.41\n117 > Best val_loss model saved: 3.402\n118 > Best val_loss model saved: 3.395\n119 > Best val_loss model saved: 3.387\n120 > Best val_loss model saved: 3.379\nepoch:120 -> train_loss=1.07028,val_loss=3.37914 - 0.0 minutes 48.0 seconds\n121 > Best val_loss model saved: 3.372\n122 > Best val_loss model saved: 3.364\n123 > Best val_loss model saved: 3.356\n124 > Best val_loss model saved: 3.348\n125 > Best val_loss model saved: 3.341\n126 > Best val_loss model saved: 3.333\n127 > Best val_loss model saved: 3.326\n128 > Best val_loss model saved: 3.318\n129 > Best val_loss model saved: 3.311\n130 > Best val_loss model saved: 3.304\nepoch:130 -> train_loss=1.05343,val_loss=3.30363 - 0.0 minutes 49.0 seconds\n131 > Best val_loss model saved: 3.296\n132 > Best val_loss model saved: 3.289\n133 > Best val_loss model saved: 3.281\n134 > Best val_loss model saved: 3.273\n135 > Best val_loss model saved: 3.265\n136 > Best val_loss model saved: 3.257\n137 > Best val_loss model saved: 3.249\n138 > Best val_loss model saved: 3.241\n139 > Best val_loss model saved: 3.233\n140 > Best val_loss model saved: 3.225\nepoch:140 -> train_loss=1.03751,val_loss=3.22466 - 0.0 minutes 49.0 seconds\n141 > Best val_loss model saved: 3.217\n142 > Best val_loss model saved: 3.209\n143 > Best val_loss model saved: 3.202\n144 > Best val_loss model saved: 3.194\n145 > Best val_loss model saved: 3.187\n146 > Best val_loss model saved: 3.18\n147 > Best val_loss model saved: 3.172\n148 > Best val_loss model saved: 3.165\n149 > Best val_loss model saved: 3.157\n150 > Best val_loss model saved: 3.15\nepoch:150 -> train_loss=1.02118,val_loss=3.15 - 0.0 minutes 48.0 seconds\n151 > Best val_loss model saved: 3.142\n152 > Best val_loss model saved: 3.135\n153 > Best val_loss model saved: 3.128\n154 > Best val_loss model saved: 3.121\n155 > Best val_loss model saved: 3.113\n156 > Best val_loss model saved: 3.106\n157 > Best val_loss model saved: 3.098\n158 > Best val_loss model saved: 3.089\n159 > Best val_loss model saved: 3.081\n160 > Best val_loss model saved: 3.071\nepoch:160 -> train_loss=1.00534,val_loss=3.07133 - 0.0 minutes 47.0 seconds\n161 > Best val_loss model saved: 3.062\n162 > Best val_loss model saved: 3.052\n163 > Best val_loss model saved: 3.042\n164 > Best val_loss model saved: 3.032\n165 > Best val_loss model saved: 3.022\n166 > Best val_loss model saved: 3.012\n167 > Best val_loss model saved: 3.002\n168 > Best val_loss model saved: 2.992\n169 > Best val_loss model saved: 2.983\n170 > Best val_loss model saved: 2.973\nepoch:170 -> train_loss=0.99109,val_loss=2.97303 - 0.0 minutes 49.0 seconds\n171 > Best val_loss model saved: 2.964\n172 > Best val_loss model saved: 2.954\n173 > Best val_loss model saved: 2.944\n174 > Best val_loss model saved: 2.935\n175 > Best val_loss model saved: 2.925\n176 > Best val_loss model saved: 2.915\n177 > Best val_loss model saved: 2.904\n178 > Best val_loss model saved: 2.894\n179 > Best val_loss model saved: 2.884\n180 > Best val_loss model saved: 2.874\nepoch:180 -> train_loss=0.97796,val_loss=2.8737 - 0.0 minutes 47.0 seconds\n181 > Best val_loss model saved: 2.864\n182 > Best val_loss model saved: 2.854\n183 > Best val_loss model saved: 2.844\n184 > Best val_loss model saved: 2.835\n185 > Best val_loss model saved: 2.825\n186 > Best val_loss model saved: 2.816\n187 > Best val_loss model saved: 2.807\n188 > Best val_loss model saved: 2.797\n189 > Best val_loss model saved: 2.787\n190 > Best val_loss model saved: 2.778\nepoch:190 -> train_loss=0.96701,val_loss=2.77778 - 0.0 minutes 48.0 seconds\n191 > Best val_loss model saved: 2.769\n192 > Best val_loss model saved: 2.759\n193 > Best val_loss model saved: 2.75\n194 > Best val_loss model saved: 2.741\n195 > Best val_loss model saved: 2.732\n196 > Best val_loss model saved: 2.723\n197 > Best val_loss model saved: 2.714\n198 > Best val_loss model saved: 2.705\n199 > Best val_loss model saved: 2.695\n200 > Best val_loss model saved: 2.686\nepoch:200 -> train_loss=0.95828,val_loss=2.68643 - 0.0 minutes 48.0 seconds\n201 > Best val_loss model saved: 2.678\n202 > Best val_loss model saved: 2.669\n203 > Best val_loss model saved: 2.66\n204 > Best val_loss model saved: 2.652\n205 > Best val_loss model saved: 2.643\n206 > Best val_loss model saved: 2.634\n207 > Best val_loss model saved: 2.626\n208 > Best val_loss model saved: 2.618\n209 > Best val_loss model saved: 2.61\n210 > Best val_loss model saved: 2.601\nepoch:210 -> train_loss=0.95142,val_loss=2.60119 - 0.0 minutes 48.0 seconds\n211 > Best val_loss model saved: 2.593\n212 > Best val_loss model saved: 2.585\n213 > Best val_loss model saved: 2.577\n214 > Best val_loss model saved: 2.569\n215 > Best val_loss model saved: 2.561\n216 > Best val_loss model saved: 2.554\n217 > Best val_loss model saved: 2.546\n218 > Best val_loss model saved: 2.539\n219 > Best val_loss model saved: 2.531\n220 > Best val_loss model saved: 2.524\nepoch:220 -> train_loss=0.94605,val_loss=2.52371 - 0.0 minutes 48.0 seconds\n221 > Best val_loss model saved: 2.517\n222 > Best val_loss model saved: 2.51\n223 > Best val_loss model saved: 2.503\n224 > Best val_loss model saved: 2.496\n225 > Best val_loss model saved: 2.489\n226 > Best val_loss model saved: 2.482\n227 > Best val_loss model saved: 2.475\n228 > Best val_loss model saved: 2.468\n229 > Best val_loss model saved: 2.462\n230 > Best val_loss model saved: 2.455\nepoch:230 -> train_loss=0.94168,val_loss=2.45525 - 0.0 minutes 51.0 seconds\n231 > Best val_loss model saved: 2.449\n232 > Best val_loss model saved: 2.443\n233 > Best val_loss model saved: 2.436\n234 > Best val_loss model saved: 2.43\n235 > Best val_loss model saved: 2.424\n236 > Best val_loss model saved: 2.418\n237 > Best val_loss model saved: 2.412\n238 > Best val_loss model saved: 2.407\n239 > Best val_loss model saved: 2.4\n240 > Best val_loss model saved: 2.395\nepoch:240 -> train_loss=0.93805,val_loss=2.39491 - 0.0 minutes 48.0 seconds\n241 > Best val_loss model saved: 2.389\n242 > Best val_loss model saved: 2.384\n243 > Best val_loss model saved: 2.378\n244 > Best val_loss model saved: 2.372\n245 > Best val_loss model saved: 2.367\n246 > Best val_loss model saved: 2.362\n247 > Best val_loss model saved: 2.357\n248 > Best val_loss model saved: 2.351\n249 > Best val_loss model saved: 2.347\n250 > Best val_loss model saved: 2.341\nepoch:250 -> train_loss=0.93479,val_loss=2.34137 - 0.0 minutes 48.0 seconds\n251 > Best val_loss model saved: 2.337\n252 > Best val_loss model saved: 2.332\n253 > Best val_loss model saved: 2.327\n254 > Best val_loss model saved: 2.322\n255 > Best val_loss model saved: 2.317\n256 > Best val_loss model saved: 2.313\n257 > Best val_loss model saved: 2.308\n258 > Best val_loss model saved: 2.303\n259 > Best val_loss model saved: 2.299\n260 > Best val_loss model saved: 2.295\nepoch:260 -> train_loss=0.93175,val_loss=2.29458 - 0.0 minutes 51.0 seconds\n261 > Best val_loss model saved: 2.29\n262 > Best val_loss model saved: 2.286\n263 > Best val_loss model saved: 2.282\n264 > Best val_loss model saved: 2.277\n265 > Best val_loss model saved: 2.273\n266 > Best val_loss model saved: 2.269\n267 > Best val_loss model saved: 2.265\n268 > Best val_loss model saved: 2.261\n269 > Best val_loss model saved: 2.257\n270 > Best val_loss model saved: 2.253\nepoch:270 -> train_loss=0.92876,val_loss=2.25277 - 0.0 minutes 51.0 seconds\n271 > Best val_loss model saved: 2.248\n272 > Best val_loss model saved: 2.245\n273 > Best val_loss model saved: 2.241\n274 > Best val_loss model saved: 2.237\n275 > Best val_loss model saved: 2.233\n276 > Best val_loss model saved: 2.229\n277 > Best val_loss model saved: 2.225\n278 > Best val_loss model saved: 2.222\n279 > Best val_loss model saved: 2.218\n280 > Best val_loss model saved: 2.214\nepoch:280 -> train_loss=0.92592,val_loss=2.21429 - 0.0 minutes 51.0 seconds\n281 > Best val_loss model saved: 2.211\n282 > Best val_loss model saved: 2.207\n283 > Best val_loss model saved: 2.203\n284 > Best val_loss model saved: 2.2\n285 > Best val_loss model saved: 2.197\n286 > Best val_loss model saved: 2.193\n287 > Best val_loss model saved: 2.19\n288 > Best val_loss model saved: 2.186\n289 > Best val_loss model saved: 2.183\n290 > Best val_loss model saved: 2.18\nepoch:290 -> train_loss=0.9232,val_loss=2.1798 - 0.0 minutes 53.0 seconds\n291 > Best val_loss model saved: 2.177\n292 > Best val_loss model saved: 2.173\n293 > Best val_loss model saved: 2.17\n294 > Best val_loss model saved: 2.167\n295 > Best val_loss model saved: 2.164\n296 > Best val_loss model saved: 2.16\n297 > Best val_loss model saved: 2.158\n298 > Best val_loss model saved: 2.154\n299 > Best val_loss model saved: 2.151\n300 > Best val_loss model saved: 2.148\nepoch:300 -> train_loss=0.92029,val_loss=2.14827 - 0.0 minutes 52.0 seconds\n301 > Best val_loss model saved: 2.145\n302 > Best val_loss model saved: 2.142\n303 > Best val_loss model saved: 2.139\n304 > Best val_loss model saved: 2.136\n305 > Best val_loss model saved: 2.133\n306 > Best val_loss model saved: 2.13\n307 > Best val_loss model saved: 2.127\n308 > Best val_loss model saved: 2.124\n309 > Best val_loss model saved: 2.122\n310 > Best val_loss model saved: 2.119\nepoch:310 -> train_loss=0.9173,val_loss=2.11881 - 0.0 minutes 52.0 seconds\n311 > Best val_loss model saved: 2.116\n312 > Best val_loss model saved: 2.113\n313 > Best val_loss model saved: 2.11\n314 > Best val_loss model saved: 2.108\n315 > Best val_loss model saved: 2.105\n316 > Best val_loss model saved: 2.102\n317 > Best val_loss model saved: 2.099\n318 > Best val_loss model saved: 2.096\n319 > Best val_loss model saved: 2.094\n320 > Best val_loss model saved: 2.091\nepoch:320 -> train_loss=0.91446,val_loss=2.09078 - 0.0 minutes 52.0 seconds\n321 > Best val_loss model saved: 2.088\n322 > Best val_loss model saved: 2.086\n323 > Best val_loss model saved: 2.083\n324 > Best val_loss model saved: 2.08\n325 > Best val_loss model saved: 2.078\n326 > Best val_loss model saved: 2.075\n327 > Best val_loss model saved: 2.073\n328 > Best val_loss model saved: 2.07\n329 > Best val_loss model saved: 2.068\n330 > Best val_loss model saved: 2.065\nepoch:330 -> train_loss=0.91151,val_loss=2.06497 - 0.0 minutes 52.0 seconds\n331 > Best val_loss model saved: 2.062\n332 > Best val_loss model saved: 2.06\n333 > Best val_loss model saved: 2.057\n334 > Best val_loss model saved: 2.055\n335 > Best val_loss model saved: 2.052\n336 > Best val_loss model saved: 2.05\n337 > Best val_loss model saved: 2.048\n338 > Best val_loss model saved: 2.045\n339 > Best val_loss model saved: 2.043\n340 > Best val_loss model saved: 2.04\nepoch:340 -> train_loss=0.9086,val_loss=2.04037 - 0.0 minutes 52.0 seconds\n341 > Best val_loss model saved: 2.038\n342 > Best val_loss model saved: 2.036\n343 > Best val_loss model saved: 2.033\n344 > Best val_loss model saved: 2.031\n345 > Best val_loss model saved: 2.028\n346 > Best val_loss model saved: 2.026\n347 > Best val_loss model saved: 2.024\n348 > Best val_loss model saved: 2.022\n349 > Best val_loss model saved: 2.02\n350 > Best val_loss model saved: 2.017\nepoch:350 -> train_loss=0.9055,val_loss=2.01739 - 0.0 minutes 51.0 seconds\n351 > Best val_loss model saved: 2.015\n352 > Best val_loss model saved: 2.013\n353 > Best val_loss model saved: 2.011\n354 > Best val_loss model saved: 2.009\n355 > Best val_loss model saved: 2.007\n356 > Best val_loss model saved: 2.005\n357 > Best val_loss model saved: 2.003\n358 > Best val_loss model saved: 2.001\n359 > Best val_loss model saved: 1.999\n360 > Best val_loss model saved: 1.997\nepoch:360 -> train_loss=0.90265,val_loss=1.99715 - 0.0 minutes 53.0 seconds\n361 > Best val_loss model saved: 1.995\n362 > Best val_loss model saved: 1.993\n363 > Best val_loss model saved: 1.991\n364 > Best val_loss model saved: 1.989\n365 > Best val_loss model saved: 1.988\n366 > Best val_loss model saved: 1.986\n367 > Best val_loss model saved: 1.984\n368 > Best val_loss model saved: 1.982\n369 > Best val_loss model saved: 1.98\n370 > Best val_loss model saved: 1.978\nepoch:370 -> train_loss=0.8995,val_loss=1.97846 - 0.0 minutes 51.0 seconds\n371 > Best val_loss model saved: 1.977\n372 > Best val_loss model saved: 1.975\n373 > Best val_loss model saved: 1.973\n374 > Best val_loss model saved: 1.972\n375 > Best val_loss model saved: 1.97\n376 > Best val_loss model saved: 1.968\n377 > Best val_loss model saved: 1.966\n378 > Best val_loss model saved: 1.964\n379 > Best val_loss model saved: 1.963\n380 > Best val_loss model saved: 1.961\nepoch:380 -> train_loss=0.89595,val_loss=1.96108 - 0.0 minutes 47.0 seconds\n381 > Best val_loss model saved: 1.959\n382 > Best val_loss model saved: 1.957\n383 > Best val_loss model saved: 1.956\n384 > Best val_loss model saved: 1.954\n385 > Best val_loss model saved: 1.952\n386 > Best val_loss model saved: 1.95\n387 > Best val_loss model saved: 1.949\n388 > Best val_loss model saved: 1.947\n389 > Best val_loss model saved: 1.945\n390 > Best val_loss model saved: 1.944\nepoch:390 -> train_loss=0.89235,val_loss=1.94372 - 0.0 minutes 47.0 seconds\n391 > Best val_loss model saved: 1.942\n392 > Best val_loss model saved: 1.94\n393 > Best val_loss model saved: 1.938\n394 > Best val_loss model saved: 1.936\n395 > Best val_loss model saved: 1.935\n396 > Best val_loss model saved: 1.933\n397 > Best val_loss model saved: 1.931\n398 > Best val_loss model saved: 1.929\n399 > Best val_loss model saved: 1.928\nFinished Training\nWall time: 5h 29min 32s\n"
    }
   ],
   "source": [
    "%%time\n",
    "# %load_ext tensorboard\n",
    "\n",
    "import time\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "from utils import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "last_loss = 0.0\n",
    "min_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "early_stop_patience = 20\n",
    "best_model = None\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "output_path = '../outputs'\n",
    "model_save_path = output_path+'/models'\n",
    "tb_path = output_path+'/logs/runs'\n",
    "\n",
    "writer = SummaryWriter(log_dir=tb_path+'/run14_sgd_1cyc_mse_lr_e-5toe-3', comment='', purge_step=None, max_queue=1, flush_secs=50, filename_suffix='')\n",
    "\n",
    "writer.add_text('run_change', 'smaller LR range:'+str(params))\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# epoch_bar = tqdm(range(epochs), ncols=12000)  # tqdm really slows down training! \n",
    "# with torch.autograd.detect_anomaly():\n",
    "for epoch in range(params['epochs']):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    stime = time.time()\n",
    "    \n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        # get the inputs; data is a list of [inputs, dist_true]\n",
    "        model.train()\n",
    "        inputs, dist_true = data[0], data[1]\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, dist_true)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        last_loss = loss.item()\n",
    "\n",
    "    val_loss = evaluate(model, val_dl)\n",
    "    lr_sched.step(val_loss)\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model = copy.deepcopy(model)\n",
    "        print(epoch,\"> Best val_loss model saved:\", round(val_loss, 3))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    train_loss = running_loss/len(train_dl)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    writer.add_scalar('loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('loss/val', val_loss, epoch)\n",
    "\n",
    "    if patience_counter > early_stop_patience:\n",
    "        print(\"Early stopping at epoch {}. current val_loss {}\".format(epoch, val_loss))\n",
    "        break\n",
    "\n",
    "    if epoch % 10 == 0:    \n",
    "        print(\"epoch:{} -> train_loss={},val_loss={} - {}\".format(epoch, round(train_loss, 5),                round(val_loss, 5), seconds_to_minutes(time.time()-stime)))\n",
    "    # epoch_bar.update(1)\n",
    "    # epoch_bar.set_description(desc=\"train_loss={},val_loss={},running_loss={}\".format(round(last_loss,3), round(val_loss,3), running_loss))\n",
    "\n",
    "print('Finished Training')\n",
    "best_model_path = model_save_path+'/model_'+str(time.time())+'.pt'\n",
    "torch.save(best_model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "([[3.6685597896575928],\n  [3.1318671703338623],\n  [2.92901349067688],\n  [3.5070533752441406],\n  [2.0010414123535156],\n  [4.5967278480529785],\n  [3.5112390518188477],\n  [3.405885696411133],\n  [3.814548969268799],\n  [4.474531173706055]],\n array([3, 3, 2, 2, 2, 3, 2, 3, 3, 3]))"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "def test(model, dl):\n",
    "    model.eval()\n",
    "    final_loss = 0.0\n",
    "    count = 0\n",
    "    y_hat = []\n",
    "    with torch.no_grad():\n",
    "        for data_cv in dl:\n",
    "            inputs, dist_true = data_cv[0], data_cv[1]\n",
    "            count += len(inputs)\n",
    "            outputs = model(inputs)\n",
    "            y_hat.extend(outputs.tolist())\n",
    "            loss = loss_fn(outputs, dist_true)\n",
    "            final_loss += loss.item()\n",
    "    return final_loss/len(dl), y_hat\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "test_loss, y_hat = test(model, test_dl)\n",
    "y_hat[0:10], y_test[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to try\n",
    "* Next try one cycle lr sched -- try 0.1 to 0.001 LR\n",
    "* Distance values of 2 & 3 dominate the data. Best way to handle?\n",
    "* try batch normalization\n",
    "* normalize outputs?\n",
    "* try Mean Absolute Error (robust to large/small outliers) and Mean Sq log error.\n",
    "* are features good enough for prediction? Check it. May be try XGBoost for it.\n",
    "* stratified data\n",
    "* check one-cycle LR and loss correlation. Log it in tensorboard.\n",
    "* print out outputs to see why loss so high?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}