{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook prepares the input data to be consumed by the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to extract the edgelist from .mtx file (the way Node2Vec expects the input). We will iterate through all the mtx files and create a corresponding .edgelist file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_files = os.listdir('../data')\n",
    "for file in data_files:\n",
    "    if file.endswith('.mtx'):\n",
    "        file_name = file.replace('.mtx', '')\n",
    "        file_edgelist = file_name+'.edgelist'\n",
    "        if not file_edgelist in data_files:\n",
    "            lines = None\n",
    "            with open('../data/'+file) as file_mtx:\n",
    "                lines = file_mtx.readlines()\n",
    "            with open('../data/'+file_edgelist, 'w') as file_edgelist:\n",
    "                file_edgelist.writelines(lines[2:])\n",
    "                print(file_edgelist, 'created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted edgelists from .mtx files using above cell, let's generate node embeddings (node2vec). For that I will use the code that the author's have shared on there [Github](https://github.com/aditya-grover/node2vec). But first we have to convert the script from python2 to python3 and replace \"import node2vec\" with \"import node2vec3\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! 2to3 -w './node2vec'  # didn't work. No such file or directory error\n",
    "# os.listdir('./node2vec')  # while this works!\n",
    "# so converted main.py and node2vec.py using online translators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generating embeddings use the parameters used in the \"shortest path distance\" paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n",
      "Wall time: 18min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "if not os.path.exists('../data/emb'):\n",
    "    os.makedirs('../data/emb')\n",
    "# ! python node2vec/main3.py --help\n",
    "! python node2vec/main3.py --input ../data/socfb-OR.edgelist --output ../data/emb/socfb-OR.emd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Graph class has a <i>naive</i> implementation of Dijkstra's Algorithm to calculate distance of all the nodes from a specified source node. It is slow but since we need to run it for landmarks (number of landmarks << number of nodes) only I will go ahead with this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0%|          | 0/150 [00:00<?, ?it/s]number of landmarks: 150\n100%|██████████| 150/150 [37:05<00:00, 14.83s/it]save path: ../outputs/distance_map_1588792161.8904061.pickle\n\n"
    }
   ],
   "source": [
    "from graph_proc import Graph\n",
    "from logger import Logger\n",
    "\n",
    "logger = Logger('../outputs/logs', 'log_')\n",
    "graph = Graph('../data/socfb-American75.mtx', logger)\n",
    "save_path = graph.process_landmarks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is saved in a pickle file (dict) to analyse. As you can see below only isolated (disconnected from source) nodes are left out, which form a cycle with another node (isolated cycles). Same set of isolated nodes are found for all of the landmarks. So we can ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of sources for which any isolated nodes found are 150\n"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "\n",
    "save_path = '../outputs/distance_map_1588792161.8904061.pickle'\n",
    "mtx_path = '..\\data\\socfb-American75.mtx'\n",
    "mat_csr = io.mmread(mtx_path).tocsr()\n",
    "distance_map = pickle.load(open(save_path, 'rb'))\n",
    "keys = list(distance_map.keys())\n",
    "count = 0\n",
    "for key in keys:\n",
    "    l = np.array(distance_map[key])\n",
    "    hitlist = np.where(l==np.inf)[0]\n",
    "    # print('Number of isolated keys for source-{} is {}'.format(key, len(hitlist)))\n",
    "    if(len(hitlist) > 0):\n",
    "        count += 1\n",
    "    # for i in hitlist:\n",
    "    #     print(i, '--', np.where(mat_csr[i].toarray()[0]>0)[0])\n",
    "    # if(len(hitlist)>0):\n",
    "    #     break\n",
    "print('Number of sources for which any isolated nodes found are', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All cells before this had to be run once to process the graph and save results to save time. Now we have to read the distance map and embeddings to form training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "size of emd_map: 0.28133392333984375 MB\nsize of distance_map: 0.00447845458984375 MB\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "save_path = '../outputs/distance_map_1588792161.8904061.pickle'\n",
    "distance_map = pickle.load(open(save_path, 'rb'))\n",
    "emd_path = '../data/emb/socfb-American75.emd'\n",
    "emd_map = {}\n",
    "with open(emd_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines[1:]:\n",
    "        temp = line.split(' ')\n",
    "        emd_map[np.int(temp[0])] = np.array(temp[1:], dtype=np.float)\n",
    "print('size of emd_map:', sys.getsizeof(emd_map)/1024/1024,'MB')\n",
    "print('size of distance_map:', sys.getsizeof(distance_map)/1024/1024,'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 150/150 [00:03<00:00, 42.47it/s]length of embedding-distance pairs 955350\n\n"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "dataset_path = '../data/datasets/socfb-American75.pickle'\n",
    "\n",
    "emd_dist_pair = []\n",
    "for landmark in tqdm(list(distance_map.keys())):\n",
    "    node_distances = distance_map[landmark]\n",
    "    emd_dist_pair.extend([((emd_map[node]+emd_map[landmark])/2, distance) for node, distance in enumerate(node_distances, 1) if node != landmark and distance != np.inf])\n",
    "\n",
    "print('length of embedding-distance pairs', len(emd_dist_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 955350/955350 [00:01<00:00, 624447.78it/s]Shape of x=(955350, 128) and y=(955350,)\nsize of x=932.9590911865234 MB and y=7.2888336181640625 MB\n\n"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "x = np.zeros((len(emd_dist_pair), len(emd_dist_pair[0][0])))\n",
    "y = np.zeros((len(emd_dist_pair),))\n",
    "\n",
    "for i, tup in enumerate(tqdm(emd_dist_pair)):\n",
    "    x[i] = tup[0]\n",
    "    y[i] = tup[1]\n",
    "print(\"Shape of x={} and y={}\".format(x.shape, y.shape))\n",
    "print('size of x={} MB and y={} MB'.format(sys.getsizeof(x)/1024/1024, sys.getsizeof(y)/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('Range of distance values', 1.0, 7.0)"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "'Range of distance values', np.min(y), np.max(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A sanity check..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "number of negative distances 0\n"
    }
   ],
   "source": [
    "num_neg_dist = 0\n",
    "distances_ = []\n",
    "for i, landmark in enumerate(distance_map.keys()):\n",
    "    distances_.extend(distance_map[landmark])\n",
    "print('number of negative distances', np.sum(np.array(distances_) < 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data takes up a lot of space, let's convert the datatype of x and y. In case you are worried about the precision loss, I think you can save the converted data into separate ndarray(x1), and try \"np.mean(np.abs(x-x1))\". For this data it was very small (2.7954226433144966e-09),so ignoring it. And in our case graphs are unweighted, so distance would be integer always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "size of x=466.47959899902344 MB and y=3.6444625854492188 MB\n"
    }
   ],
   "source": [
    "x = x.astype('float32')\n",
    "y = y.astype('int')\n",
    "print('size of x={} MB and y={} MB'.format(sys.getsizeof(x)/1024/1024, sys.getsizeof(y)/1024/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "shapes of train, validation, test data (573209, 128) (573209,) (143303, 128) (143303,) (238838, 128) (238838,)\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "seed_random = 9999\n",
    "np.random.seed(seed_random)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_random)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, train_size=0.75, random_state=seed_random)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, test_size=0.2, train_size=0.8, random_state=seed_random)\n",
    "\n",
    "print('shapes of train, validation, test data', x_train.shape, y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discribed in point-2 in fun.ipyb, the distance values are heavily skewed. So oversampling under-represented samples. But here is the challenge-- how do you oversample a regression dataset? Didn't find any good existing implementations. But in this cases values are all natural numbers and these might as well be classes! So using SMOTE. \n",
    "Note: I am not using classification models because distance might be outside the range of [[1-7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Frequency of distance values before oversampling [1 2 3 4 5 6 7] [  6509 201940 305724  53701   4890    420     25]\nFrequency of distance values after oversampling (array([1, 2, 3, 4, 5, 6, 7]), array([ 91717, 201940, 305724,  91717,  91717,  91717,  91717],\n      dtype=int64))\n"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "values, counts = np.unique(y_train, return_counts=True)\n",
    "print('Frequency of distance values before oversampling', values, counts)\n",
    "max_val = np.max(counts)\n",
    "resample_dict = {1: int(max_val*0.3), 4:int(max_val*0.3), 5:int(max_val*0.3), 6:int(max_val*0.3), 7:int(max_val*0.3)}\n",
    "\n",
    "ros = RandomOverSampler(sampling_strategy=resample_dict, random_state=seed_random)\n",
    "x_train, y_train = ros.fit_resample(x_train, y_train.astype(np.int))\n",
    "\n",
    "print('Frequency of distance values after oversampling', np.unique(y_train, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# TODO try standardization and no normalization also and compare result\n",
    "\n",
    "mm_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_train = mm_scaler.fit_transform(x_train)\n",
    "x_cv = mm_scaler.transform(x_cv)\n",
    "x_test = mm_scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2400\n",
    "input_size = x_train.shape[1]\n",
    "hidden_units_1 = 256\n",
    "hidden_units_2 = 100\n",
    "output_size = 1\n",
    "\n",
    "lr = 1e-2\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "device: cuda:0\n"
    }
   ],
   "source": [
    "from torch.utils import data as torch_data\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "trainset = torch_data.TensorDataset(torch.as_tensor(x_train, dtype=torch.float, device=device), torch.as_tensor(y_train, dtype=torch.float, device=device))\n",
    "train_dl = torch_data.DataLoader(trainset, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "val_dl = torch_data.DataLoader(torch_data.TensorDataset(torch.as_tensor(x_cv, dtype=torch.float, device=device), torch.as_tensor(y_cv, dtype=torch.float, device=device)), batch_size=batch_size, drop_last=True)\n",
    "\n",
    "test_dl = torch_data.DataLoader(torch_data.TensorDataset(torch.as_tensor(x_test, dtype=torch.float, device=device), torch.as_tensor(y_test, dtype=torch.float, device=device)), batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "model loaded into device= cuda:0\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Linear-1                  [-1, 256]          33,024\n              ReLU-2                  [-1, 256]               0\n            Linear-3                  [-1, 100]          25,700\n              ReLU-4                  [-1, 100]               0\n            Linear-5                    [-1, 1]             101\n              ReLU-6                    [-1, 1]               0\n================================================================\nTotal params: 58,825\nTrainable params: 58,825\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 0.22\nEstimated Total Size (MB): 0.23\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "torch.manual_seed(9999)\n",
    "def get_model():\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(input_size, hidden_units_1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_units_1, hidden_units_2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_units_2, output_size),\n",
    "        torch.nn.ReLU(),\n",
    "        # torch.nn.Softplus(),\n",
    "    )\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def poissonLoss(xbeta, y):\n",
    "    \"\"\"Custom loss function for Poisson model.\"\"\"\n",
    "    loss=torch.mean(torch.exp(xbeta)-y*xbeta)\n",
    "    return loss\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "print('model loaded into device=', next(model.parameters()).device)\n",
    "summary(model, input_size=(128, ))\n",
    "\n",
    "lr_reduce_patience = 10\n",
    "lr_reduce_factor = 0.05\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, dampening=0, weight_decay=0, nesterov=True)\n",
    "lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_reduce_factor, patience=lr_reduce_patience, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=1e-8, eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dl):\n",
    "    model.eval()\n",
    "    final_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for data_cv in dl:\n",
    "            inputs, dist_true = data_cv[0], data_cv[1]\n",
    "            count += len(inputs)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, dist_true)\n",
    "            final_loss += loss.item()\n",
    "    return final_loss/len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 > Best val_loss model saved: 18.853\nepoch:0 -> train_loss=0.4278,val_loss=18.85271 - 0.0 minutes 34.0 seconds\n1 > Best val_loss model saved: 18.835\n2 > Best val_loss model saved: 18.647\nepoch:10 -> train_loss=0.76341,val_loss=18.72356 - 0.0 minutes 30.0 seconds\nEpoch    14: reducing learning rate of group 0 to 5.0000e-04.\n14 > Best val_loss model saved: 5.689\n15 > Best val_loss model saved: 5.584\n16 > Best val_loss model saved: 5.583\n17 > Best val_loss model saved: 5.582\n18 > Best val_loss model saved: 5.582\nepoch:20 -> train_loss=2.97424,val_loss=5.58234 - 0.0 minutes 31.0 seconds\n21 > Best val_loss model saved: 5.582\n22 > Best val_loss model saved: 5.582\n23 > Best val_loss model saved: 5.582\n24 > Best val_loss model saved: 5.582\n25 > Best val_loss model saved: 5.582\n26 > Best val_loss model saved: 5.582\nEpoch    28: reducing learning rate of group 0 to 2.5000e-05.\n27 > Best val_loss model saved: 5.582\n28 > Best val_loss model saved: 4.494\n29 > Best val_loss model saved: 3.698\n30 > Best val_loss model saved: 3.112\nepoch:30 -> train_loss=3.97375,val_loss=3.11247 - 0.0 minutes 31.0 seconds\n31 > Best val_loss model saved: 2.677\n32 > Best val_loss model saved: 2.351\n33 > Best val_loss model saved: 2.103\n34 > Best val_loss model saved: 1.914\n35 > Best val_loss model saved: 1.768\n36 > Best val_loss model saved: 1.654\n37 > Best val_loss model saved: 1.565\n38 > Best val_loss model saved: 1.495\n39 > Best val_loss model saved: 1.439\n40 > Best val_loss model saved: 1.395\nepoch:40 -> train_loss=3.1677,val_loss=1.39507 - 0.0 minutes 31.0 seconds\n41 > Best val_loss model saved: 1.36\n42 > Best val_loss model saved: 1.331\n43 > Best val_loss model saved: 1.308\n44 > Best val_loss model saved: 1.29\n45 > Best val_loss model saved: 1.275\n46 > Best val_loss model saved: 1.263\n47 > Best val_loss model saved: 1.253\n48 > Best val_loss model saved: 1.245\n49 > Best val_loss model saved: 1.238\n50 > Best val_loss model saved: 1.233\nepoch:50 -> train_loss=3.14202,val_loss=1.23298 - 0.0 minutes 31.0 seconds\n51 > Best val_loss model saved: 1.229\n52 > Best val_loss model saved: 1.225\n53 > Best val_loss model saved: 1.222\n54 > Best val_loss model saved: 1.22\n55 > Best val_loss model saved: 1.218\n56 > Best val_loss model saved: 1.216\n57 > Best val_loss model saved: 1.215\n58 > Best val_loss model saved: 1.214\n59 > Best val_loss model saved: 1.213\n60 > Best val_loss model saved: 1.213\nepoch:60 -> train_loss=3.14005,val_loss=1.2126 - 0.0 minutes 31.0 seconds\n61 > Best val_loss model saved: 1.212\n62 > Best val_loss model saved: 1.212\n63 > Best val_loss model saved: 1.211\n64 > Best val_loss model saved: 1.211\n65 > Best val_loss model saved: 1.211\n66 > Best val_loss model saved: 1.21\n67 > Best val_loss model saved: 1.21\n68 > Best val_loss model saved: 1.21\n69 > Best val_loss model saved: 1.21\n70 > Best val_loss model saved: 1.21\nepoch:70 -> train_loss=3.13981,val_loss=1.20993 - 0.0 minutes 31.0 seconds\n71 > Best val_loss model saved: 1.21\n72 > Best val_loss model saved: 1.21\n73 > Best val_loss model saved: 1.21\n74 > Best val_loss model saved: 1.21\n75 > Best val_loss model saved: 1.21\n76 > Best val_loss model saved: 1.21\n77 > Best val_loss model saved: 1.21\n78 > Best val_loss model saved: 1.21\n79 > Best val_loss model saved: 1.21\n80 > Best val_loss model saved: 1.21\nepoch:80 -> train_loss=3.13978,val_loss=1.20957 - 0.0 minutes 31.0 seconds\n81 > Best val_loss model saved: 1.21\n82 > Best val_loss model saved: 1.21\n83 > Best val_loss model saved: 1.21\n84 > Best val_loss model saved: 1.21\n85 > Best val_loss model saved: 1.21\n86 > Best val_loss model saved: 1.21\n87 > Best val_loss model saved: 1.21\n88 > Best val_loss model saved: 1.21\n89 > Best val_loss model saved: 1.21\n90 > Best val_loss model saved: 1.21\nepoch:90 -> train_loss=3.13978,val_loss=1.20951 - 0.0 minutes 31.0 seconds\n91 > Best val_loss model saved: 1.21\n92 > Best val_loss model saved: 1.21\nEpoch    94: reducing learning rate of group 0 to 1.2500e-06.\n93 > Best val_loss model saved: 1.21\n94 > Best val_loss model saved: 1.209\n95 > Best val_loss model saved: 1.208\n96 > Best val_loss model saved: 1.207\n97 > Best val_loss model saved: 1.206\n98 > Best val_loss model saved: 1.205\n99 > Best val_loss model saved: 1.204\nFinished Training\nWall time: 51min 17s\n"
    }
   ],
   "source": [
    "%%time\n",
    "# %load_ext tensorboard\n",
    "\n",
    "import time\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "from utils import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "last_loss = 0.0\n",
    "min_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "early_stop_patience = 20\n",
    "best_model = None\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "output_path = '../outputs'\n",
    "model_save_path = output_path+'/models'\n",
    "tb_path = output_path+'/logs/runs'\n",
    "\n",
    "writer = SummaryWriter(log_dir=tb_path+'/run4_4l_oversamp', comment='', purge_step=None, max_queue=1, flush_secs=20, filename_suffix='')\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# epoch_bar = tqdm(range(epochs), ncols=12000)  # tqdm really slows down training! \n",
    "# with torch.autograd.detect_anomaly():\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    stime = time.time()\n",
    "    \n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        # get the inputs; data is a list of [inputs, dist_true]\n",
    "        model.train()\n",
    "        inputs, dist_true = data[0], data[1]\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, dist_true)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        last_loss = loss.item()\n",
    "\n",
    "    val_loss = evaluate(model, val_dl)\n",
    "    lr_sched.step(val_loss)\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model = copy.deepcopy(model)\n",
    "        print(epoch,\"> Best val_loss model saved:\", round(val_loss, 3))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    train_loss = running_loss/len(train_dl)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    writer.add_scalar('loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('loss/val', val_loss, epoch)\n",
    "\n",
    "    if patience_counter > early_stop_patience:\n",
    "        print(\"Early stopping at epoch {}. current val_loss {}\".format(epoch, val_loss))\n",
    "        break\n",
    "\n",
    "    if epoch % 10 == 0:    \n",
    "        print(\"epoch:{} -> train_loss={},val_loss={} - {}\".format(epoch, round(train_loss, 5),                round(val_loss, 5), seconds_to_minutes(time.time()-stime)))\n",
    "    # epoch_bar.update(1)\n",
    "    # epoch_bar.set_description(desc=\"train_loss={},val_loss={},running_loss={}\".format(round(last_loss,3), round(val_loss,3), running_loss))\n",
    "\n",
    "print('Finished Training')\n",
    "best_model_path = model_save_path+'/model_'+str(time.time())+'.pt'\n",
    "torch.save(best_model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "([[3.59720778465271],\n  [3.59720778465271],\n  [3.59720778465271],\n  [3.59720778465271],\n  [3.59720778465271],\n  [3.59720778465271],\n  [3.59720778465271],\n  [3.59720778465271],\n  [3.59720778465271],\n  [3.59720778465271]],\n array([3, 2, 3, 2, 3, 2, 3, 3, 3, 2]))"
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "def test(model, dl):\n",
    "    model.eval()\n",
    "    final_loss = 0.0\n",
    "    count = 0\n",
    "    y_hat = []\n",
    "    with torch.no_grad():\n",
    "        for data_cv in dl:\n",
    "            inputs, dist_true = data_cv[0], data_cv[1]\n",
    "            count += len(inputs)\n",
    "            outputs = model(inputs)\n",
    "            y_hat.extend(outputs.tolist())\n",
    "            loss = loss_fn(outputs, dist_true)\n",
    "            final_loss += loss.item()\n",
    "    return final_loss/len(dl), y_hat\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "test_loss, y_hat = test(model, test_dl)\n",
    "y_hat[0:10], y_test[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to try\n",
    "* Next try one cycle lr sched\n",
    "* Distance values of 2 & 3 dominate the data. Best way to handle?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}